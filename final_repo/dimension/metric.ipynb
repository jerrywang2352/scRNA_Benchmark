{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import FastICA\n",
    "import umap\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, silhouette_score\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expression_data():\n",
    "   data = pd.read_csv('../data/data.csv')\n",
    "   data = data.T\n",
    "   data = data.drop(data.index[:3])\n",
    "   return data\n",
    "\n",
    "def pca(data):\n",
    "    '''PCA'''\n",
    "    # Select only the expression data columns for PCA\n",
    "    expression_data = data\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(expression_data)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=2)  # You can change the number of components as needed\n",
    "    principal_components = pca.fit_transform(scaled_data)\n",
    "    return principal_components,scaled_data\n",
    "\n",
    "def tsne(data):\n",
    "    selected_columns = data\n",
    "    tsne = TSNE(n_components=2, perplexity=30)  # Adjust parameters as needed\n",
    "\n",
    "    # Perform t-SNE\n",
    "    tsne_result = tsne.fit_transform(selected_columns)\n",
    "    return tsne_result\n",
    "\n",
    "def Umap(data):\n",
    "    selected_columns = data  \n",
    "    umap_reducer = umap.UMAP(n_components=2)  \n",
    "\n",
    "    umap_result = umap_reducer.fit_transform(selected_columns)\n",
    "    return umap_result\n",
    "\n",
    "def ica(data):\n",
    "    gene_expression = data\n",
    "    ica = FastICA(n_components=2, random_state=42)\n",
    "\n",
    "\n",
    "    ica.fit(gene_expression)\n",
    "\n",
    "    independent_components = ica.transform(gene_expression)\n",
    "    return independent_components\n",
    "\n",
    "def vae(data):\n",
    "    expression_data = data.values\n",
    "\n",
    "\n",
    "    expression_data = (expression_data - np.min(expression_data)) / (np.max(expression_data) - np.min(expression_data))\n",
    "    expression_data = tf.convert_to_tensor(expression_data, dtype=tf.float32)\n",
    "    latent_dim = 2 \n",
    "\n",
    "    encoder_inputs = keras.Input(shape=(expression_data.shape[1],))\n",
    "    x = keras.layers.Dense(256, activation='relu')(encoder_inputs)\n",
    "    x = keras.layers.Dense(128, activation='relu')(x)\n",
    "    z_mean = keras.layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = keras.layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(tf.shape(z_mean)[0], latent_dim), mean=0., stddev=1.)\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = keras.layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    decoder_inputs = keras.layers.Dense(128, activation='relu')(z)\n",
    "    decoder_outputs = keras.layers.Dense(expression_data.shape[1], activation='sigmoid')(decoder_inputs)\n",
    "\n",
    "    vae = keras.Model(encoder_inputs, decoder_outputs)\n",
    "\n",
    "    reconstruction_loss = tf.keras.losses.mean_squared_error(encoder_inputs, decoder_outputs)\n",
    "    reconstruction_loss *= expression_data.shape[1]\n",
    "    kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "    kl_loss = tf.reduce_mean(kl_loss)\n",
    "    kl_loss *= -0.5\n",
    "    vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n",
    "    vae.add_loss(vae_loss)\n",
    "    vae.compile(optimizer='adam')\n",
    "\n",
    "    vae.fit(expression_data, epochs=10, batch_size=32)\n",
    "\n",
    "    encoder = keras.Model(encoder_inputs, z_mean)\n",
    "    encoded_data = encoder.predict(expression_data)\n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: -0.9964476021314387\n",
      "Normalized Mutual Information (NMI): 0.0002426712335969816\n",
      "Adjusted Rand Index (ARI) for PCA: -0.0017793594306049821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS Software\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#ARI for PCA\n",
    "num_clusters = 2 # Change this to the number of clusters you want\n",
    "pca_result,scaled_data = pca(expression_data())\n",
    "\n",
    "# Perform K-means clustering\n",
    "kmeans1 = KMeans(n_clusters=num_clusters)  # Create a KMeans instance\n",
    "cluster_labels1 = kmeans1.fit_predict(scaled_data)  # Fit the model and obtain cluster labels\n",
    "\n",
    "kmeans2 = KMeans(n_clusters=num_clusters)  # Create a KMeans instance\n",
    "cluster_labels2 = kmeans2.fit_predict(scaled_data)  # Fit the model and obtain cluster labels\n",
    "ari = adjusted_rand_score(cluster_labels1,cluster_labels2)\n",
    "nmi = normalized_mutual_info_score(cluster_labels1,cluster_labels2)\n",
    "silhouette_avg = silhouette_score(cluster_labels1.reshape(-1, 1),cluster_labels2.reshape(-1, 1))\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "print(f\"Normalized Mutual Information (NMI): {nmi}\")\n",
    "print(f\"Adjusted Rand Index (ARI) for PCA: {ari}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS Software\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "d:\\CS Software\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 1.0\n",
      "Normalized Mutual Information (NMI): 1.0\n",
      "Adjusted Rand Index (ARI) for PCA: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS Software\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#ARI for TSNE\n",
    "\n",
    "num_clusters = 2 # Change this to the number of clusters you want\n",
    "scaled_data = tsne(expression_data())\n",
    "# Perform K-means clustering\n",
    "kmeans1 = KMeans(n_clusters=num_clusters)  # Create a KMeans instance\n",
    "cluster_labels1 = kmeans1.fit_predict(scaled_data)  # Fit the model and obtain cluster labels\n",
    "\n",
    "kmeans2 = KMeans(n_clusters=num_clusters)  # Create a KMeans instance\n",
    "cluster_labels2 = kmeans2.fit_predict(scaled_data)  # Fit the model and obtain cluster labels\n",
    "ari = adjusted_rand_score(cluster_labels1,cluster_labels2)\n",
    "nmi = normalized_mutual_info_score(cluster_labels1,cluster_labels2)\n",
    "silhouette_avg = silhouette_score(cluster_labels1.reshape(-1, 1),cluster_labels2.reshape(-1, 1))\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "print(f\"Normalized Mutual Information (NMI): {nmi}\")\n",
    "print(f\"Adjusted Rand Index (ARI) for PCA: {ari}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 1.0\n",
      "Normalized Mutual Information (NMI): 1.0\n",
      "Adjusted Rand Index (ARI) for PCA: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS Software\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#ARI for UMAP\n",
    "\n",
    "num_clusters = 2 # Change this to the number of clusters you want\n",
    "scaled_data = Umap(expression_data())\n",
    "# Perform K-means clustering\n",
    "kmeans1 = KMeans(n_clusters=num_clusters)  # Create a KMeans instance\n",
    "cluster_labels1 = kmeans1.fit_predict(scaled_data)  # Fit the model and obtain cluster labels\n",
    "\n",
    "kmeans2 = KMeans(n_clusters=num_clusters)  # Create a KMeans instance\n",
    "cluster_labels2 = kmeans2.fit_predict(scaled_data)  # Fit the model and obtain cluster labels\n",
    "ari = adjusted_rand_score(cluster_labels1,cluster_labels2)\n",
    "nmi = normalized_mutual_info_score(cluster_labels1,cluster_labels2)\n",
    "silhouette_avg = silhouette_score(cluster_labels1.reshape(-1, 1),cluster_labels2.reshape(-1, 1))\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "print(f\"Normalized Mutual Information (NMI): {nmi}\")\n",
    "print(f\"Adjusted Rand Index (ARI) for PCA: {ari}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.9788811576254338\n",
      "Normalized Mutual Information (NMI): 0.9271112837454577\n",
      "Adjusted Rand Index (ARI) for PCA: 0.9669992110758683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS Software\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#ARI for ICA\n",
    "num_clusters = 2 # Change this to the number of clusters you want\n",
    "scaled_data = ica(expression_data())\n",
    "# Perform K-means clustering\n",
    "kmeans1 = KMeans(n_clusters=num_clusters)  # Create a KMeans instance\n",
    "cluster_labels1 = kmeans1.fit_predict(scaled_data)  # Fit the model and obtain cluster labels\n",
    "\n",
    "kmeans2 = KMeans(n_clusters=num_clusters)  # Create a KMeans instance\n",
    "cluster_labels2 = kmeans2.fit_predict(scaled_data)  # Fit the model and obtain cluster labels\n",
    "ari = adjusted_rand_score(cluster_labels1,cluster_labels2)\n",
    "nmi = normalized_mutual_info_score(cluster_labels1,cluster_labels2)\n",
    "silhouette_avg = silhouette_score(cluster_labels1.reshape(-1, 1),cluster_labels2.reshape(-1, 1))\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "print(f\"Normalized Mutual Information (NMI): {nmi}\")\n",
    "print(f\"Adjusted Rand Index (ARI) for PCA: {ari}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18/18 [==============================] - 6s 236ms/step - loss: 13202.2461\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 4s 227ms/step - loss: 9372.1670\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 4s 222ms/step - loss: 3036.6580\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 4s 227ms/step - loss: 394.2442\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 4s 228ms/step - loss: 219.3492\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 4s 216ms/step - loss: 147.0635\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 4s 221ms/step - loss: 142.7207\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 4s 220ms/step - loss: 125.9234\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 4s 223ms/step - loss: 170.0518\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 4s 222ms/step - loss: 129.5052\n",
      "18/18 [==============================] - 0s 9ms/step\n",
      "Silhouette Score: 1.0\n",
      "Normalized Mutual Information (NMI): 1.0\n",
      "Adjusted Rand Index (ARI) for PCA: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS Software\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#ARI for VAE\n",
    "num_clusters = 2 # Change this to the number of clusters you want\n",
    "scaled_data = vae(expression_data())\n",
    "# Perform K-means clustering\n",
    "kmeans1 = KMeans(n_clusters=num_clusters)  # Create a KMeans instance\n",
    "cluster_labels1 = kmeans1.fit_predict(scaled_data)  # Fit the model and obtain cluster labels\n",
    "\n",
    "kmeans2 = KMeans(n_clusters=num_clusters)  # Create a KMeans instance\n",
    "cluster_labels2 = kmeans2.fit_predict(scaled_data)  # Fit the model and obtain cluster labels\n",
    "ari = adjusted_rand_score(cluster_labels1,cluster_labels2)\n",
    "nmi = normalized_mutual_info_score(cluster_labels1,cluster_labels2)\n",
    "silhouette_avg = silhouette_score(cluster_labels1.reshape(-1, 1),cluster_labels2.reshape(-1, 1))\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "print(f\"Normalized Mutual Information (NMI): {nmi}\")\n",
    "print(f\"Adjusted Rand Index (ARI) for PCA: {ari}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
